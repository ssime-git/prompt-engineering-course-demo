{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Ollama\n",
    "\n",
    "This notebook demonstrates how to use Jupyter_ai with Ollama for prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection status: 200\n",
      "Available models:\n",
      "- openthinker:latest\n",
      "- llama3.2:latest\n",
      "- nomic-embed-text:latest\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://host.docker.internal:11434/api/tags\")\n",
    "    print(f\"Connection status: {response.status_code}\")\n",
    "    print(\"Available models:\")\n",
    "    for model in response.json().get('models', []):\n",
    "        print(f\"- {model.get('name')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the jupyter_ai_magics extension\n",
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n"
      ],
      "text/plain": [
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available AI models\n",
    "%ai list ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Effective Prompt Engineering Techniques\n",
       "#### 1. **Clear and Specific Instructions**\n",
       "====\n",
       "\n",
       "*   Define a clear goal or task for the model.\n",
       "*   Provide specific details about the context, tone, and style.\n",
       "*   Use concrete examples to illustrate the desired outcome.\n",
       "\n",
       "Example:\n",
       "```markdown\n",
       "Write a persuasive essay on climate change, targeting a high school audience. Assume the student has no prior knowledge of the topic. Include statistics and supporting evidence from reputable sources.\n",
       "```\n",
       "\n",
       "#### 2. **Contextualizing the Prompt**\n",
       "====\n",
       "\n",
       "*   Provide relevant background information or context.\n",
       "*   Specify any assumptions or constraints that might affect the model's response.\n",
       "*   Use contextual keywords to help the model understand the task.\n",
       "\n",
       "Example:\n",
       "```markdown\n",
       "Assume you are a product manager at a tech startup. Write a press release announcing the launch of your new eco-friendly smartphone. Provide details about the phone's features, pricing, and availability. Include quotes from the CEO or other key team members.\n",
       "```\n",
       "\n",
       "#### 3. **Guiding the Model with Constraints**\n",
       "====\n",
       "\n",
       "*   Impose constraints on the model's response to encourage creativity or focus.\n",
       "*   Use specific formatting requirements, such as length or word count limits.\n",
       "*   Define acceptable responses based on user preferences.\n",
       "\n",
       "Example:\n",
       "```markdown\n",
       "Write a poem about love, using only 14 lines and following a strict rhyme scheme. Include the words \"heart\" and \"soul\". Assume a formal tone and use poetic language to convey emotions.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "What are three effective prompt engineering techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Understanding LLM Parameter Tuning Techniques\n",
       "\n",
       "#### Top-P (Top Probability)\n",
       "====\n",
       "\n",
       "*   **What it does:** The model generates the most likely response based on the input prompt.\n",
       "*   **Example:**\n",
       "    *   Input Prompt: \"Write a short story about a character who discovers a hidden world within their own reflection.\"\n",
       "        +   With `top_p=0.95`, the model might generate a response like: \"As she gazed into the mirror, she saw her reflection fade away, revealing a mystical realm where stars and planets danced in perfect harmony.\"\n",
       "            *   This response is coherent and follows the original prompt's intent.\n",
       "    *   However, it may lack creativity or originality.\n",
       "\n",
       "#### Top-K (Top Candidates)\n",
       "====\n",
       "\n",
       "*   **What it does:** The model generates the top K most likely responses based on the input prompt, rather than just the single most likely one.\n",
       "*   **Example:**\n",
       "    *   Input Prompt: \"Write a short story about a character who discovers a hidden world within their own reflection.\"\n",
       "        +   With `top_k=5`, the model might generate two additional responses:\n",
       "            1.  \"As she gazed into the mirror, she saw her reflection transform into a wispy forest, where trees bore fruit that tasted like moonlight and dreams.\"\n",
       "            2.  \"In the glass, she found a city of ancient mirrors, their reflections whispering secrets of the past to those who dared to listen.\"\n",
       "    *   These responses add diversity and creativity while still being coherent.\n",
       "\n",
       "#### Temperature\n",
       "====\n",
       "\n",
       "*   **What it does:** Controls how \"soft\" or \"hard\" the model generates responses.\n",
       "*   **Example:**\n",
       "    *   Input Prompt: \"Write a short story about a character who discovers a hidden world within their own reflection.\"\n",
       "        +   With `temperature=1.0`, the model might generate a response like: \"There was once a person who discovered a magical realm inside their mirror.\"\n",
       "            *   This response is general and lacks specificity.\n",
       "    *   However, if you increase the temperature to 1.2 or more, the model might produce a more creative answer:\n",
       "        +   \"In a dimension hidden within the labyrinthine corridors of mirrors, a young dreamer discovered an enchanted realm where emotions took on lives of their own.\"\n",
       "    *   Higher temperatures promote more creative and general responses.\n",
       "\n",
       "These parameters help you fine-tune your language model's performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "explain LLM parameter top_p, top_k, temperature\n",
    "This is for beginners. Use specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Effective Prompt Engineering Techniques\n",
       "\n",
       "#### 1. **Clear and Specific Language**\n",
       "Using clear and specific language can help ensure that the model understands what you want it to do. Avoid vague or open-ended prompts, and instead focus on specific tasks or actions.\n",
       "\n",
       "#### 2. **Repetition and Reinforcement**\n",
       "Repeating and reinforcing certain words, phrases, or concepts within a prompt can help the model learn and understand its intended meaning. This technique is particularly effective for models that are trained using reinforcement learning.\n",
       "\n",
       "#### 3. **Analogical Reasoning**\n",
       "Using analogies to explain complex concepts or tasks can help models understand abstract ideas and relationships between objects. By providing an analogy, you can provide a concrete example of what you want the model to learn, making it easier for them to generalize and apply that knowledge."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "What are three effective prompt engineering techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| **Approach** | **Code Generation** | **Creative Writing** | **Factual Answers** |\n",
       "| --- | --- | --- | --- |\n",
       "| **Clear and Specific Language** | Encourages precise specifications, reducing ambiguity | Fosters clarity on tone, style, and narrative direction | Emphasizes accuracy in factual information |\n",
       "| **Repetition and Reinforcement** | Helps models learn specific syntax patterns and coding conventions | Enhances consistency in writing style and tone | Validates fact-based knowledge through repetition |\n",
       "| **Analogical Reasoning** | Facilitates understanding of complex algorithms and data structures | Aids in creative problem-solving by mapping analogies to narrative structures | Illuminates relationships between concepts, improving factual accuracy |\n",
       "| **Counterfactuals** | Allows exploration of alternative scenarios and edge cases | Explores multiple narrative paths and character development | Examines hypothetical situations for fact-checking |\n",
       "| **Self-Questioning** | Encourages model introspection on coding assumptions | Inspires writers to question their own biases and assumptions | Fosters critical thinking in evaluating factual information |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "Create a comparison table of different prompt engineering approaches for:\n",
    "1. Code generation\n",
    "2. Creative writing\n",
    "3. Factual answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
